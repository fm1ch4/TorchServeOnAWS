{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving PyTorch Models In Production Natively With Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Your Hosting Environment\n",
    "The focus of this lab is around model serving. In that vain, we have taken care of of the data preparation and model training. \n",
    "This lab exercise is using a [HuggingFace Transformer](https://huggingface.co/transformers/) which provides us with a general-purpose architecture for Natural Language Understanding (NLU). Specifically, we are presenting you with a [RoBERTa base](https://huggingface.co/roberta-base) transformer that was fined tuned to perform sentiment analysis. The pre-trained checkpoint loads the additional head layers and will output ``positive``, ``neutral``, and ``negative`` sentiment or text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "#This is the fine-tuned roberta-base transformer hosted on an S3 bucket.\n",
    "# Have we considered a bring-your-own-model example instead of using a pre-trained model?\n",
    "# That way, we can also showcase the model archiver tool of TorchServe and how we expect model artifacts to be packaged for deployment on SageMaker\n",
    "model_artifact = 's3://torchserve-workshop/roberta-fine-tuned.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Your Endpoint\n",
    "We will now create and deploy our model. To begin, we need to construct a new PyTorchModel object which points to the pre-trained model artifacts from the above step and also points to the inference code that we wish to use. We will then call the deploy method to launch the deployment container on our TorchServe powered Amazon SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, \n",
    "                         deserializer=json_deserializer, content_type='application/json')\n",
    "\n",
    "model = PyTorchModel(model_data=model_artifact,\n",
    "                   name=name_from_base('roberta-model'),\n",
    "                   role=role, \n",
    "                   # is it worth having a cell where we 'cat' this file and explain the programming interface?\n",
    "                   # that way, customers can modify it as needed based on the model they want to use.\n",
    "                   entry_point='torchserve-predictor.py',\n", 
    "                   source_dir='source_dir',\n",
    "                   framework_version='1.5.0',\n",
    "                   predictor_cls=SentimentAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take 6-8 minutes for your TorchServe powered endpoint to spin up on Amazon SageMaker \n",
    "\n",
    "# Here we are setting the endpoint name so we can delete it later using Boto3\n",
    "endpoint_name = name_from_base('roberta-model')\n",
    "print(endpoint_name)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Predictions With A TorchServe Backend SageMaker Endpoint\n",
    "Here, we will pass sample strings of text to the endpoint in order to see the sentiment. We give you one example of each, however, feel free to play around and change the strings yourself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our endpoint should predict a positive sentiment from the text below\n",
    "test_data = {\"text\": \"AWS is excited to announce that TorchServe is natively supported in Amazon SageMaker as the default model server for PyTorch inference\"}\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Review text: {test_data}')\n",
    "print(f'Sentiment  : {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our endpoint should predict a neutral sentiment from the text below\n",
    "test_data = {\"text\": \"TorchServe addresses an industry need.\"}\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Review text: {test_data}')\n",
    "print(f'Sentiment  : {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our endpoint should predict a negative sentiment from the text below\n",
    "test_data = {\"text\": \"I never liked having to convert my models just to deploy them in production!\"}\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Review text: {test_data}')\n",
    "print(f'Sentiment  : {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Cleanup: Delete Endpoint\n",
    "In order to ensure that we are no longer being billed for the endpoint that we have spun up, we use the below step to tear it down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see a 'HTTPStatusCode': 200 then your endpoint has been sucessfully deleted. \n",
    "# You can also verify this from within the AWS console by navigating to the Amazon SageMaker service and clicking Endpoints.\n",
    "\n",
    "sm.delete_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "Please head back to the workshop to learn more about the next lab. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
